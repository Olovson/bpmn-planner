# Codex Pipeline Status

## ‚úÖ Vad som fungerar

1. **Samma prompt**: Codex anv√§nder exakt samma prompt som ChatGPT/Ollama via `getPromptForDocType()`
2. **Samma struktur**: Codex anv√§nder `buildLlmRequestStructure()` - samma funktion som ChatGPT/Ollama
3. **Samma mapping**: Codex anv√§nder `mapLlmResponseToModel()` - samma funktion som ChatGPT/Ollama
4. **Terminal-kommandon**: Fungerar via `npm run codex:batch:auto`

## ‚ùå Vad som INTE fungerar (k√§nd begr√§nsning)

**Full kontext i batch-kontexten**: 

`buildFullNodeContext()` fungerar INTE i Node.js batch-kontexten eftersom:
- `parseBpmnFile()` anv√§nder `fetch()` f√∂r web-URLs (`/bpmn/...`)
- `fetch()` fungerar inte med filsystem-paths i Node.js
- D√§rf√∂r fallback till minimal kontext i batch-kontexten

## üîß L√∂sning som beh√∂vs

F√∂r att f√• full kontext i batch-kontexten beh√∂ver vi:

1. **Node.js-variant av `parseBpmnFile`** som l√§ser fr√•n filsystemet ist√§llet f√∂r web-URLs
2. **Eller en adapter** som konverterar filsystem-paths till web-URLs och hanterar filsystem-l√§sning

## üìä Nuvarande beteende

- **Batch-kontext (terminal)**: Anv√§nder minimal kontext (endast nod-metadata)
- **Web-kontext (app)**: ChatGPT/Ollama anv√§nder full kontext (hierarki, flows, relaterade noder)

## ‚ö†Ô∏è Varning i genererat inneh√•ll

N√§r minimal kontext anv√§nds l√§ggs en varning till i `userPrompt` som syns i genererat inneh√•ll:

```
‚ö†Ô∏è OBS: Denna dokumentation genererades med minimal kontext (endast nod-metadata). 
Full kontext med hierarki, flows och relaterade noder kunde inte byggas i batch-kontexten. 
F√∂r b√§sta kvalitet, generera via ChatGPT/Ollama-pipelinen i appen d√§r full kontext √§r tillg√§nglig.
```

## üéØ Rekommendation

F√∂r b√§sta kvalitet:
- **Anv√§nd ChatGPT/Ollama-pipelinen i appen** f√∂r full kontext
- **Anv√§nd Codex batch** f√∂r snabb initial generering med minimal kontext
- **Granska och f√∂rb√§ttra** genererat inneh√•ll manuellt eller via ChatGPT/Ollama

